{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourabh111dotcom/3rd-Capstone-CHD-Prediction-/blob/main/Cardiovascular_Risk_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Cardiovascular Risk Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name -** Sourabh Choudhary\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know Coronary Heart Disease (CHD) is one of the major problems in todays world. First of all for doing Classification problem we have to import all the necessary libraries which are used to complete our project. Then we will mount our drive to load our dataset. Our dataset contains following features:\n",
        "\n",
        "*   **id** : It contains unique Id of patients from 0 to 3390.\n",
        "*   **age** : It contains the age of patients for which we have to predict CHD.\n",
        "\n",
        "*   **education** : It contains Education of the patients between 1,2,3,4.\n",
        "*   **sex** : It contains the gender of the patients whether the patient is Male or Female.\n",
        "\n",
        "*   **is_smoking** : This columns contains whether the patient is smoking or not. So values in it is in either YES or NO.\n",
        "*   **cigsPerDay** : It contains the quantity of cigarette the patient consumes per day.\n",
        "\n",
        "*   **BPmeds** : It contains whether the patient is taking BP Medicine or not. Here, 1 means patient is taking BP medicines and 0 means patient is not taking medicines.\n",
        "*   **prevalentStroke** : It contains whether the patient has history of stroke or not. Here, if it is yes then it should be 1 or if no then it should be 0.\n",
        "\n",
        "*   **prevalentHyp** : It contains whether the patient has history of hypertension or not. Here, 1 denotes he has hypertension before and 0 denotes the patient doesnot have hypertension.\n",
        "*   **diabetes** : It contains whether the patient has diabetes or not. Here, 1 means patient has diabetes and 0 means patient doesnot have diabetes.\n",
        "\n",
        "*   **totChol** : It contains the measure of the cholestrol of the patients.\n",
        "*   **sysBP** : It contains systollic Blood Pressure measure of the patients.\n",
        "\n",
        "*   **diaB**P : It contains diastolic Blood Pressure measure of the patients.\n",
        "*   **BMI** : It contains Body Mass Index of the patients.\n",
        "\n",
        "*   **heartRate** : It contains the heart rate of the patients.\n",
        "*   **glucose** : It contains the glucose level of the patients.\n",
        "\n",
        "*   **TenYearCHD** : It contains whether the patients whether a patient has a 10 year risk of future coronary heart disease(CHD).\n",
        "\n",
        "Our dataset contains 17 columns and 3390 rows in which there are no duplicate values but  there are some missing values, education contain 87 missing values, cigsPerDay contains 22 missing values, BPMeds contains 44 missing values, totChol contains 38 missing values, BMI contains 14 missing values, heartRate contains 1 missing value and glucose contains 304 missing values. For handling the missing value I did Data Cleaning.Data cleaning was done in following steps:\n",
        "\n",
        "*   Id was not a important feature to take it in modelling. So, I dropped whole column.\n",
        "*   Then I converted sex which was in M for Male and F for Female into '1' for male and '0' for female. Then I converted is_smoking which was YES and NO into '1' for yes '0' for No. \n",
        "\n",
        "*   I imputed missing values of Education and BPMeds  with mode.\n",
        "\n",
        "*   Cigs_Per_Day contains outliers so I have imputed median values in the missing places of cigsPerDay.\n",
        "*   Here, totChol also contains outliers so I imputed it with median values.\n",
        "\n",
        "*   As heartRate contains only 1 missing values so I dropped that row.\n",
        "*   Glucose has lot of outliers so we can impute data with median but imputing such a large data will give us a high bias, so to maintain that I used KNN imputer.\n",
        "\n",
        "After that I applied Exploratory Data Analysis on my cleaned data. First I did Univariate Analysis in which I found BPMeds, prevalentStroke and diabetes are poorly balaced.\n",
        "\n",
        "Then in visualization of Dependent Variable I found TenYearCHD is poorly imbalanced. And in visualization of Independent Variable it is found that eduaction contains between 1 to 4 , females have more chances of CHD, half of the patients are smokers, 100 patients only take BP medicines,only 22 patients have experianced stroke, 1069 patients have hypertension and 87 patients have diabetes. \n",
        "After visualization I found following inferences:\n",
        "\n",
        "\n",
        "*   Percentage of CHD for educdation are- 1(18%),2(11%),3(12%) and 4(14%).\n",
        "*   Male have relatively high chance of CHD(18%) than female(12%).\n",
        "*   Smokers have more chances of CHD(16%).\n",
        "*   Patients taking BP medicines have chances of CHD(33%).\n",
        "*   Patients having stroke in past have high chances of CHD(45%).\n",
        "*   Patient having hypertension are have more chances of CHD(23%).\n",
        "*   Patients having diabetes are more prone towards CHD(37%).\n",
        "\n",
        "After that I found correlation in which I found that systollic BP and diastollic BP is highly correlated to each other.So, to maintain is multicollinearity I converted sysBP and diaBP in one column using Pulse Pressure= Systollic Pressure - Diastollic Pressure.\n",
        "\n",
        "After that I did Feature selection for that I found Chi 2 value and p - value test. P - value of prevalent stroke is low it means it is the most important feauture in the dataset and is_smoking p- value is highest among all so, it means it is not a good feature for modelling so, I dropped that column.\n",
        "\n",
        "Then our data was skewed so, I applied log transformation. Then I took X = TenYearCHD as dependendent variable and otheres y = independent variable. Then I applied train_test_split() on the dataset after that I used recall as the evaluation matrics in it.As our data is poorly imbalanced so we need to oversample it. So, for overesampling I used SMOTE. Then for scaling our data I used StandardScaler().After that I applied different Classification models such as:\n",
        "\n",
        "*   First of all I applied KNN model I found False negative is 35, I found train_recall 0.91 and test_recall 0.65 so there a large difference between train and test recall.\n",
        "*   Then I applied Logistic Regression where I found False negative is 41 which is higher than KNN. Then train_recall and test_recall of Logistic Regression are 0.71 and 0.59 which is not good as compared to previous model.\n",
        "\n",
        "*   Then I applied Naive Bayes and I found that False negative is 69 and train_recall and test_recall are 0.42 and 0.32 so, we can say that Naive Bayes is not good model for our data.\n",
        "*   Then I applied Support Vector Machines and I found that is 39 and train_recall and test_recall are 0.71 and 0.61 which is a good model but not better than KNN.\n",
        "\n",
        "*   Then I applied Decision Tree and I found False negative is 19 and train_recall and test_recall are 0.84 and 0.81. Here we can see it is lowest false negative and best train_recall and best test_recall among all it Decision tree was the best model among all.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -** https://github.com/sourabh111dotcom/3rd-Capstone-CHD-Prediction-\n",
        "\n"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts.\n",
        "The classification goal is to predict whether the patient has a 10-year risk of future coronary heart disease (CHD).\n",
        "The dataset provides the patients’ information. It includes over 4,000 records and 15 attributes. Variables Each attribute is a potential risk factor. There are both demographic, behavioral, and medical risk factors.** "
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix as cm\n",
        "from sklearn.metrics import ConfusionMatrixDisplay as cmd\n",
        "import xgboost as xgb\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set() "
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "77WvAdNCuLQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/classification project/data_cardiovascular_risk.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Dataset contains 3390 rows and 17 columns."
      ],
      "metadata": {
        "id": "0Puikf1tvrub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset contains data of many feautures/columns for which we have to predict whether a patient has a 10 year risk of future coronary heart disease(CHD).\n",
        "\n",
        "The following columns are:\n",
        "\n",
        "\n",
        "*   **Id** : It contains unique Id of patients from 0 to 3390.\n",
        "*   **Age** : It contains the age of patients for which we have to predict CHD.\n",
        "\n",
        "*   **Education** : It has Education of the patients 1,2,3,4.  \n",
        "*   **Sex** : It contains the gender of the patients whether the patient is Male or Female.\n",
        "\n",
        "*   **is_smoking** : This columns contains whether the patient is smoking or not. So values in it is in either YES or NO.\n",
        "*   **cigsPerDay** : It contains the quantity of cigarette the patient consumes per day. \n",
        "\n",
        "*   **BPmeds** : It contains whether the patient is taking BP Medicine or not. Here, 1 means patient is taking BP medicines and 0 means patient is not taking medicines. \n",
        "*   **prevalentStroke** : It  contains whether the patient has history of stroke or not. Here, if it is yes then it should be 1 or if no then it should be 0.\n",
        "\n",
        "*   **prevalentHyp** : It contains whether the patient has history of hypertension or not. Here, 1 denotes he has hypertension before and 0 denotes the patient doesnot have hypertension.\n",
        "*   **Diabetes** : It contains whether the patient has diabetes or not. Here, 1 means patient has diabetes and 0  means patient doesnot have diabetes.\n",
        "\n",
        "*   **totChol** : It contains the measure of the cholestrol of the patients.\n",
        "*   **sysBP** : It contains systollic Blood Pressure measure of the patients.\n",
        "\n",
        "*   **diaBP** : It contains diastolic Blood Pressure measure of the patients.\n",
        "*   **BMI** : It contains Body Mass Index of the patients.\n",
        "\n",
        "*   **heartRate** : It contains the heart rate of the patients. \n",
        "*   **glucose** : It contains the glucose level of the patients.\n",
        "\n",
        "*   **TenYearCHD** : It contains whether the patients whether a patient has a 10 year risk of future coronary heart disease(CHD).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XPvAHg8yzXvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate = df.duplicated()\n",
        "print(duplicate.value_counts())\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we get result as false means our dataset doesnot contain any duplicate data."
      ],
      "metadata": {
        "id": "zpCpGeX1IHVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is many null values in our dataset. we can see clearly that education contains 87 null values, cigsPerDay contain 22 null values, BPMeds contain 44 null values, totChol contains 38 null values, BMI contain 14 null values, heartRate contain 1 null value and glucose contain 304 null values."
      ],
      "metadata": {
        "id": "gqfSiFNbIp_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.heatmap(df.isnull(), cbar=True, yticklabels=False)\n",
        "plt.xlabel(\"Column_name\", size=12, weight=\"bold\")\n",
        "plt.title(\"Missing values\",fontweight=\"bold\",size=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From our correlation heatmap we can say that education, cigsPerDay, BPMeds, totChol, BMI,heartRate, glucose are the columns which has missing values."
      ],
      "metadata": {
        "id": "ThGL_tOdHN8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data Cleaning***"
      ],
      "metadata": {
        "id": "ZDrHUg3jHt9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying data to preserve orignal dataset\n",
        "new_df = df.copy()"
      ],
      "metadata": {
        "id": "oswo390uHzJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping 'id' as it is not required\n",
        "new_df.drop(columns=['id'],inplace=True)"
      ],
      "metadata": {
        "id": "zEUmTDW9KWJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the binary columns\n",
        "new_df['sex'] = np.where(new_df['sex'] == 'M',1,0)\n",
        "new_df['is_smoking'] = np.where(new_df['is_smoking'] == 'YES',1,0)"
      ],
      "metadata": {
        "id": "F0yL8Dn_K3Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "id": "oxMAAKn8LrGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Here, we can see we have dropped 'id' column which is not required.\n",
        "*   Here we have converted sex column where Male = '1' and Female = '0'.\n",
        "\n",
        "*   And we have converted is_smoking column where YES = '1' and NO = '0'.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ln-TYTIfMFhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Replacing the missing values in Education**"
      ],
      "metadata": {
        "id": "c-evCxpmRI9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing the missing values in the Education columns with its mode\n",
        "new_df['education'] = new_df['education'].fillna(new_df['education'].mode()[0])\n"
      ],
      "metadata": {
        "id": "02DnrpusMCyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Replacing the missing values in BPMeds**"
      ],
      "metadata": {
        "id": "dRjeYYLQYTBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing the missing values in the BPMeds columns with its mode\n",
        "new_df['BPMeds'] = new_df['BPMeds'].fillna(new_df['BPMeds'].mode()[0])"
      ],
      "metadata": {
        "id": "8CaYob-xZIyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Replacing the missing values in cigsPerDay**"
      ],
      "metadata": {
        "id": "P1DpIE5VnXvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All missing values in the cigsPerDay column\n",
        "new_df[new_df['cigsPerDay'].isna()]"
      ],
      "metadata": {
        "id": "cUCQWjpKvF_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this we can say that all the missing values in cigsPerDay are smoking daily."
      ],
      "metadata": {
        "id": "SZ7jb5KYwaqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of no. of cigarettes per day for smokers \n",
        "plt.figure(figsize=(8,4))\n",
        "sns.distplot(new_df[new_df['is_smoking']==1]['cigsPerDay'])\n",
        "plt.axvline(new_df[new_df['is_smoking']==1]['cigsPerDay'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(new_df[new_df['is_smoking']==1]['cigsPerDay'].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "plt.title('Cigarette per day  distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nApjdLp2xUQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above visualization we can say that both mean and median are close to each other so, we will check outliers for proper imputting the data in the missing places."
      ],
      "metadata": {
        "id": "Ju8-v20cyGhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# box plot for the no. of cigarettes per day for smokers \n",
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(new_df[new_df['is_smoking']==1]['cigsPerDay'])"
      ],
      "metadata": {
        "id": "PWzvWDSzyija"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above visualization we can see that there are some outliers in this column so we will impute the median value in the missing places."
      ],
      "metadata": {
        "id": "WACiIvPYy7p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing the missing values in the cigsPerDay \n",
        "new_df['cigsPerDay'] = new_df['cigsPerDay'].fillna(new_df[new_df['is_smoking']==1]['cigsPerDay'].median())"
      ],
      "metadata": {
        "id": "MssA0O8l0W6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Replacing the missing values in totChol**"
      ],
      "metadata": {
        "id": "W2BrRAwW1oKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of total cholestrol of the patient \n",
        "plt.figure(figsize=(8,4))\n",
        "sns.distplot(new_df['totChol'])\n",
        "plt.axvline(new_df['totChol'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(new_df['totChol'].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "plt.title('Total Cholestrol of the Patient')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Wgjq34R1-6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will check outliers in the column."
      ],
      "metadata": {
        "id": "t1w9hGs05lP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# box plot for total cholestrol of the patient\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(new_df['totChol'])"
      ],
      "metadata": {
        "id": "sXhu3up252gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see that this totChol column contains outliers so we will impute median in the missing places."
      ],
      "metadata": {
        "id": "NMQhbe9U6i8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing missing values in the totChol with their medain values\n",
        "new_df['totChol'] = new_df['totChol'].fillna(new_df['totChol'].median())"
      ],
      "metadata": {
        "id": "-azTIf3n63cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Replacing the missing values in BMI**"
      ],
      "metadata": {
        "id": "x6BTo1Z67scX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of BMI of the patient \n",
        "plt.figure(figsize=(8,4))\n",
        "sns.distplot(new_df['BMI'])\n",
        "plt.axvline(new_df['BMI'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(new_df['BMI'].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "plt.title('BMI of the Patient')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AFH_sf4H8idL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After seeing that both mean and median are very close we will check outliers in our column."
      ],
      "metadata": {
        "id": "GAzZj1iw810b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# box plot for BMI of the patient\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(new_df['BMI'])"
      ],
      "metadata": {
        "id": "sTKxCdI_9F_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this boxplot we can say that there are lot outliers so we will impute median in the misssing places."
      ],
      "metadata": {
        "id": "qoxnS75b9SjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing missing values in the BMI with their medain values\n",
        "new_df['BMI'] = new_df['BMI'].fillna(new_df['BMI'].median())"
      ],
      "metadata": {
        "id": "7uWC9Epf-MGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dropping the missing value in heartRate**"
      ],
      "metadata": {
        "id": "kR8_xDyj_E_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As heartRate contains only 1 missing value so, we can easily drop that row because it would not affect our model."
      ],
      "metadata": {
        "id": "osYBpAw6_W6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the missing values from column heartRate\n",
        "new_df=new_df[new_df['heartRate'].notna()]"
      ],
      "metadata": {
        "id": "BGpY0Owu_sUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Replacing the missing values in glucose**"
      ],
      "metadata": {
        "id": "IKLzVRuQEkjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of glucose\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.distplot(new_df['glucose'])\n",
        "plt.axvline(new_df['glucose'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(new_df['glucose'].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "plt.title('Glucose distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0dNpcLcvE6al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean and Median are close enough and the graph is positively skewed so we will check for outliers."
      ],
      "metadata": {
        "id": "ds4r3iFNFw1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# box plot for glucose\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(new_df['glucose'])"
      ],
      "metadata": {
        "id": "8MXm0HzqGPJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, glucose column contains lot of outliers so, we can impute median values in missing places but glucose column contain 304 missing values which is a great number if we will impute median value then we will get very high bias.\n",
        "\n",
        "So, to avoid this we can impute the missing values using KNN imputer."
      ],
      "metadata": {
        "id": "KNKVis0WGfZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using KNN imputer with K=10\n",
        "imputer = KNNImputer(n_neighbors=10)\n",
        "imputed = imputer.fit_transform(new_df)\n",
        "new_df = pd.DataFrame(imputed, columns=new_df.columns)"
      ],
      "metadata": {
        "id": "yqFpUXXZIl5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking our dataset after applying KNN imputer\n",
        "new_df.info()"
      ],
      "metadata": {
        "id": "AM-7iQscJucC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see KNN imputer has coverted every value into float so we will convert it accordingly."
      ],
      "metadata": {
        "id": "mOZomEykJ-SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing datatypes of the following columns\n",
        "new_df = new_df.astype({'age': int, 'education':int,'sex':int,'is_smoking':int,'cigsPerDay':int,\n",
        "               'BPMeds':int,'prevalentStroke':int,'prevalentHyp':int,'diabetes':int,\n",
        "               'totChol':float,'sysBP':float,'diaBP':float,\n",
        "               'BMI':float,'heartRate':float,'glucose':float,'TenYearCHD':int})\n",
        "     "
      ],
      "metadata": {
        "id": "qqaGlqPXKUNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "new_df.isna().sum()"
      ],
      "metadata": {
        "id": "nhv3ZOj7PHUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, all our missing values are managed and we are ready to do EDA."
      ],
      "metadata": {
        "id": "xLbvFgNIPkyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "tFCZrdLfcD6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Univariate Analysis**"
      ],
      "metadata": {
        "id": "2VlmP-Gf14f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# understanding distribution of data before imputation\n",
        "fig = plt.figure(figsize = (15,20))\n",
        "ax = fig.gca()\n",
        "new_df.hist(ax = ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "96ccmOnnahXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, BPMeds, prevalent stroke and diabetes are poorly imbalanced."
      ],
      "metadata": {
        "id": "y3dhRcRUgbaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorizing different features in dependent, continous and categorical variables.\n",
        "dependent_var = ['TenYearCHD']\n",
        "continuous_var = ['age','cigsPerDay','totChol','sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n",
        "categorical_var = ['education', 'sex', 'is_smoking','BPMeds','prevalentStroke', 'prevalentHyp', 'diabetes']"
      ],
      "metadata": {
        "id": "PYTJl833iSIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Distribution of Dependent Variable**"
      ],
      "metadata": {
        "id": "6CC6siTejoxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of dependent varaible \n",
        "plt.figure(figsize=(8,4))\n",
        "sns.countplot(df[dependent_var[0]])\n",
        "plt.xlabel(dependent_var[0])\n",
        "plt.title(dependent_var[0]+' distribution')"
      ],
      "metadata": {
        "id": "rmelY8ImjxB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see TenYearCHD is poorly imbalanced."
      ],
      "metadata": {
        "id": "bkOmjK17lKva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From our above visualisation we got following inferences:\n",
        "\n",
        "*   Maximum education category of people is 1 followed by 2,3 and 4.\n",
        "*   There are more female patients than male.\n",
        "\n",
        "*   Almost half of the patient are smokers.\n",
        "*   Only  100 patients are taking BP Medicines.\n",
        "\n",
        "*   Only 22 patient have experienced a stroke.\n",
        "*   1069 patients have hypertension.\n",
        "\n",
        "*   87 patients have diabetes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aA5t9SIviuVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bivariate Analysis**"
      ],
      "metadata": {
        "id": "z_NTPRg8M9N6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analyzing the relationship between the Dependent Variable and the Continuous Variables**\n"
      ],
      "metadata": {
        "id": "34KdB87ZmWQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Relationship between the dependent variable and continuous independent variables\n",
        "for i in continuous_var:\n",
        "  plt.figure(figsize=(8,4))\n",
        "  sns.catplot(x=dependent_var[0],y=i,data=new_df,kind='violin')\n",
        "  plt.ylabel(i)\n",
        "  plt.xlabel(dependent_var[0])\n",
        "  plt.title(dependent_var[0]+' vs '+i)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "0NqHi0kVmVe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this we can say that as the age increases changes of CHD increases."
      ],
      "metadata": {
        "id": "x11I7S2-p95T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analyzing the relationship between the Dependent Variable and the Discrete Variables**"
      ],
      "metadata": {
        "id": "xJzCwKGPrKW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing the relationship between the dependent variable and categorical independent variables\n",
        "for i in categorical_var:\n",
        "  plt.figure(figsize=(8,4))\n",
        "  sns.histplot(x=i, hue=dependent_var[0], data=new_df, stat=\"count\", multiple=\"stack\")\n",
        "  plt.title('Risk of CHD by: '+i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "W-anrIS5sIjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this data is unevenly distributed and this graph doesnot give any conclusive inference."
      ],
      "metadata": {
        "id": "jHO0LEE4tCGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stacked bar chart\n",
        "\n",
        "for i in categorical_var:\n",
        "    x_var, y_var = i, dependent_var[0]\n",
        "    plt.figure(figsize=(8,4))\n",
        "    df_grouped = new_df.groupby(x_var)[y_var].value_counts(normalize=True).unstack(y_var)*100\n",
        "    df_grouped.plot.barh(stacked=True)\n",
        "    plt.legend(\n",
        "        bbox_to_anchor=(1.05, 1),\n",
        "        loc=\"upper left\",\n",
        "        title=y_var)\n",
        "\n",
        "    plt.title(\"% of patients at the risk of CHD by: \"+i)\n",
        "    for ix, row in df_grouped.reset_index(drop=True).iterrows():\n",
        "        # print(ix, row)\n",
        "        cumulative = 0\n",
        "        for element in row:\n",
        "            if element > 0.1:\n",
        "                plt.text(\n",
        "                    cumulative + element / 2,\n",
        "                    ix,\n",
        "                    f\"{int(element)} %\",\n",
        "                    va=\"center\",\n",
        "                    ha=\"center\",\n",
        "                )\n",
        "            cumulative += element\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1V_pH-WDujq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the following graph we conclude the following inferences:\n",
        "\n",
        "*   Percentage of CHD for educdation are- 1(18%),2(11%),3(12%) and 4(14%).\n",
        "*   Male have relatively high chance of CHD(18%) than female(12%).\n",
        "\n",
        "*   Smokers have more chances of CHD(16%).\n",
        "*   Patients taking BP medicines have chances of CHD(33%).\n",
        "\n",
        "*   Patients having stroke in past have high chances of CHD(45%).\n",
        "*   Patient having hypertension are have more chances of CHD(23%).\n",
        "\n",
        "*   Patients having diabetes are more prone towards CHD(37%).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_AGDT7hYvxSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Correlation analysis**"
      ],
      "metadata": {
        "id": "lTJ9ghDxNYP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation magnitude\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title('Correlation Analysis')\n",
        "correlation = new_df[continuous_var].corr()\n",
        "sns.heatmap(abs(correlation), annot=True)"
      ],
      "metadata": {
        "id": "Em7MuwwpNXXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can clearly see that systolic BP and diastolic BP is highly correlated to each other."
      ],
      "metadata": {
        "id": "MkicG4ASOfZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handling Multicollinearity**"
      ],
      "metadata": {
        "id": "GYoE6AR3PSqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Range of systolic bp and diastolic bp\n",
        "print(new_df['sysBP'].min(),new_df['sysBP'].max())\n",
        "print(new_df['diaBP'].min(),new_df['diaBP'].max())"
      ],
      "metadata": {
        "id": "tXpKeC6HOtm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, range of systolic BP is from 83.5 to 295 and range of diastolic BP is 48 to 142.5.\n",
        "\n",
        "So, we can handle this multicollinearty by changing sysBP and diaBP into one column. It can be done by:\n",
        "\n",
        "Pulse Pressure = systolic BP - diastolic BP"
      ],
      "metadata": {
        "id": "8TFB6DkGQFf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column pulse_pressure and dropping sysBP and diaBP\n",
        "\n",
        "new_df['pulse_pressure'] = new_df['sysBP']-new_df['diaBP']\n",
        "new_df.drop('sysBP',axis=1,inplace=True)\n",
        "new_df.drop('diaBP',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "v3ofiK24RJvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking our columns\n",
        "new_df.columns"
      ],
      "metadata": {
        "id": "ZNpSUDGvRhX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the continuous_var list\n",
        "continuous_var.remove('sysBP')\n",
        "continuous_var.remove('diaBP')\n",
        "continuous_var.append('pulse_pressure')"
      ],
      "metadata": {
        "id": "wP5fDNYtSGo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing the distribution of pulse_pressure\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.distplot(new_df['pulse_pressure'])\n",
        "plt.axvline(new_df['pulse_pressure'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(new_df['pulse_pressure'].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "plt.title('Pulse Pressure Distribution')"
      ],
      "metadata": {
        "id": "1d6mNrMTSTDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relationship between pulse pressure with the dependent variable\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.catplot(x=dependent_var[0],y='pulse_pressure',data=new_df,kind='violin')\n",
        "plt.title('tenYearCHD vs pulse_pressure')\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "eSrFX60zSpsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see patients with higher pulse_pressure have more chances of CHD."
      ],
      "metadata": {
        "id": "xfzs8qk8S2F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After removing collinearity\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title('Correlation Analysis')\n",
        "correlation = new_df[continuous_var].corr()\n",
        "sns.heatmap(abs(correlation), annot=True)"
      ],
      "metadata": {
        "id": "P8dlrqYXT04Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we have removed collinearity in our data."
      ],
      "metadata": {
        "id": "6ZyW52KfUEF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection**"
      ],
      "metadata": {
        "id": "Cq_GC_uAVKUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection one of the important part for modelling. So, here we will use chi2 and p-value test for feature selection."
      ],
      "metadata": {
        "id": "b44UuJdJVVS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chi2 scores\n",
        "chi_scores = chi2(new_df[categorical_var],new_df[dependent_var])\n",
        "chi_scores"
      ],
      "metadata": {
        "id": "oCv3bH72UKNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# P values for discrete features\n",
        "p_values = pd.Series(chi_scores[1],index = new_df[categorical_var].columns)\n",
        "p_values.sort_values(ascending = False , inplace = True)\n",
        "p_values"
      ],
      "metadata": {
        "id": "15ejHZsoWwAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting p values for chi2 test for discrete features\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.xscale('log')\n",
        "plt.xlabel('P-value')\n",
        "plt.title('P-value for discrete features')\n",
        "p_values.plot.barh()"
      ],
      "metadata": {
        "id": "HLC-RxpjXG3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, prevalentHyp has lowest p-value which means it is most important feautes in categorical variable.\n",
        "\n",
        "And is_smoking has highest p-value which means this feauture is least important as cigeratte per day us the required so is_smoking here is useless feature. "
      ],
      "metadata": {
        "id": "GNxcgJs-XZSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping is_smoking\n",
        "new_df.drop('is_smoking',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "uGiCML_ymMpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping is smoking \n",
        "categorical_var.remove('is_smoking')\n",
        "categorical_var"
      ],
      "metadata": {
        "id": "kDVrO6iCmW29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transforming Continuous Variables to reduce skew**"
      ],
      "metadata": {
        "id": "WuNS_9IinAH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# skewness along the index axis\n",
        "(new_df[continuous_var]).skew(axis = 0)"
      ],
      "metadata": {
        "id": "c_eGemPCm9IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can clearly see our continous variable are skewed. So, we can reduce our skewness by doing log transformation."
      ],
      "metadata": {
        "id": "4_PfJ5jXoEqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying log transformation\n",
        "np.log10(new_df[continuous_var]+1).skew(axis = 0)"
      ],
      "metadata": {
        "id": "ShRKeaH5oaxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing log transformation on continuous variables\n",
        "\n",
        "new_df['age']                   = np.log10(new_df['age']+1)\n",
        "new_df['cigsPerDay']          = np.log10(new_df['cigsPerDay']+1)\n",
        "new_df['totChol']       =       np.log10(new_df['totChol']+1)\n",
        "new_df['BMI']                   = np.log10(new_df['BMI']+1)\n",
        "new_df['heartRate']            = np.log10(new_df['heartRate']+1)\n",
        "new_df['glucose']               = np.log10(new_df['glucose']+1)\n",
        "new_df['pulse_pressure']        = np.log10(new_df['pulse_pressure']+1)"
      ],
      "metadata": {
        "id": "SM6YxcoKo53d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking skew after log transformation\n",
        "new_df[continuous_var].skew(axis = 0)"
      ],
      "metadata": {
        "id": "BxnpqT6eqA8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analyzing the Distribution of Transformed Features**"
      ],
      "metadata": {
        "id": "LGCbEpppth9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysing the distribution of continuous varaibles after transformation\n",
        "for col in continuous_var:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  sns.distplot(new_df[col])\n",
        "  plt.axvline(new_df[col].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "  plt.axvline(new_df[col].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "  plt.title(col+' distribution')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JeaRge-LtVYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see after aplying log transformation our features are normally distributed."
      ],
      "metadata": {
        "id": "PSTM3MHfuJB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "guRcq1nmv-ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining dependent and independent variables\n",
        "X = new_df.drop('TenYearCHD',axis=1)\n",
        "y = new_df[dependent_var]"
      ],
      "metadata": {
        "id": "bLlNDHE7wBRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y, shuffle=True)"
      ],
      "metadata": {
        "id": "0kC8T0vBxJea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation Metrics**"
      ],
      "metadata": {
        "id": "YVDeYjkED7fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get recall score\n",
        "def recall(actual,predicted):\n",
        "  '''\n",
        "  recall(actual,predicted)\n",
        "  '''\n",
        "  return recall_score(y_true=actual, y_pred=predicted, average='binary')"
      ],
      "metadata": {
        "id": "QfS7RChID--y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Oversampling**"
      ],
      "metadata": {
        "id": "ZjSeiVSNxtyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the target variable before SMOTE\n",
        "y_train.value_counts().plot(kind='bar', title='Target variable before SMOTE')"
      ],
      "metadata": {
        "id": "SB9Mm-JExxzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data is imbalanced to balance it we have to apply SMOTE."
      ],
      "metadata": {
        "id": "UcOCMMq_yBPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "\n",
        "# fit predictor and target variable\n",
        "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print('Samples in the original dataset', len(y_train))\n",
        "print('Samples in the resampled dataset', len(y_smote))"
      ],
      "metadata": {
        "id": "pXv50Onqy9Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the target variable after SMOTE\n",
        "y_smote.value_counts().plot(kind='bar', title='Target variable after SMOTE')"
      ],
      "metadata": {
        "id": "xNUJQx4SzEpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scaling the Data**"
      ],
      "metadata": {
        "id": "haAUQ92W8EDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_smote_scaled = scaler.fit_transform(X_smote)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Converting array to dataframe\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled,columns=X_train.columns)\n",
        "X_smote_scaled = pd.DataFrame(X_smote_scaled,columns=X_smote.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled,columns=X_test.columns)"
      ],
      "metadata": {
        "id": "6UWAStgU8DsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled train values\n",
        "X_train_scaled.head()"
      ],
      "metadata": {
        "id": "zNnAqyIu89PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaled SMOTE values\n",
        "X_smote_scaled.head()"
      ],
      "metadata": {
        "id": "moWWSYJw9gvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaled test values\n",
        "X_test_scaled.head()"
      ],
      "metadata": {
        "id": "i1NHfizu9lJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we have successfully scaled down the variables using standard scaler."
      ],
      "metadata": {
        "id": "dgDqZggB-Cht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **K Nearest Neighbors**"
      ],
      "metadata": {
        "id": "jXlJKV4eBP0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbours is one of the most basic yet essential classification algorithms in Machine Learning. It belongs to the supervised learning domain and finds intense application in pattern recognition, data mining and intrusion detection.\n",
        "It is widely disposable in real-life scenarios since it is non-parametric, meaning, it does not make any underlying assumptions about the distribution of data (as opposed to other algorithms such as GMM, which assume a Gaussian distribution of the given data)."
      ],
      "metadata": {
        "id": "Id6fLQGuBTDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Value of k taken upto sqrt(n)\n",
        "# Where n is no of records in the train dataset\n",
        "# sqrt(3390) = 58.2\n",
        "knn_test_res = []\n",
        "knn_train_res = []\n",
        "for k in range(1,60):\n",
        "  knn_model = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn_model.fit(X_smote_scaled, y_smote)\n",
        "  knn_train_pred = knn_model.predict(X_smote_scaled)\n",
        "  knn_train_recall = recall(y_smote,knn_train_pred)\n",
        "  knn_test_pred = knn_model.predict(X_test_scaled)\n",
        "  knn_test_recall = recall(y_test,knn_test_pred)\n",
        "  knn_test_res.append(knn_test_recall)\n",
        "  knn_train_res.append(knn_train_recall)\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "N8kuZTeJBj32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the train and test recalls for different values of k\n",
        "plt.figure(figsize=(8,4))\n",
        "x_ = range(1,60)\n",
        "y1 = knn_train_res\n",
        "y2 = knn_test_res\n",
        "plt.plot(x_, y1, label='Train Recall')\n",
        "plt.plot(x_, y2, label = 'Test Recall')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GVNXef4bE_1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best k is where the test recall is the highest\n",
        "best_k = knn_test_res.index(max(knn_test_res))+1\n",
        "best_k"
      ],
      "metadata": {
        "id": "vyirCGxNFdGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building knn model with best parameters\n",
        "knn_model = KNeighborsClassifier(n_neighbors=best_k)"
      ],
      "metadata": {
        "id": "akVsf4RvFlDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "knn_model.fit(X_smote_scaled, y_smote)"
      ],
      "metadata": {
        "id": "4U1m9DLkFnqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train predictions\n",
        "knn_train_pred = knn_model.predict(X_smote_scaled)"
      ],
      "metadata": {
        "id": "e-LOJWxrFzlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training set recall\n",
        "knn_train_recall = recall(y_smote,knn_train_pred)\n",
        "knn_train_recall"
      ],
      "metadata": {
        "id": "trEhl6WEGKjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test predictions\n",
        "knn_test_pred = knn_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "6VsjY_ehGSZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test recall\n",
        "knn_test_recall = recall(y_test,knn_test_pred)\n",
        "knn_test_recall"
      ],
      "metadata": {
        "id": "ulnQFmZKGWlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test,knn_test_pred))"
      ],
      "metadata": {
        "id": "9-obBDJNHS9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "knn_confusion_matrix = cm(y_test, knn_test_pred)\n",
        "cm_display = cmd(confusion_matrix = knn_confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "font = {'family' : 'DejaVu Sans',\n",
        "        'weight' : 'bold',\n",
        "        'size'   : 22}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "cm_display.plot(cmap='Blues')\n",
        "plt.title('Confusion matrix: KNN')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KvgBAdFpHhhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see False Negative after applying KNN is 35."
      ],
      "metadata": {
        "id": "87Jk3QFXwN70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**"
      ],
      "metadata": {
        "id": "V14WMOsc0VeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1."
      ],
      "metadata": {
        "id": "2bTB0F2f4BU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting logistic regression model\n",
        "lr_model = LogisticRegression()"
      ],
      "metadata": {
        "id": "aeBST8OL1vE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "lr_model.fit(X_smote_scaled, y_smote)"
      ],
      "metadata": {
        "id": "Lq_9ZBtJ15_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train predictions\n",
        "lr_train_pred = lr_model.predict(X_smote_scaled)"
      ],
      "metadata": {
        "id": "Qpz3iu542CII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set recall\n",
        "lr_train_recall = recall(y_smote,lr_train_pred)\n",
        "lr_train_recall"
      ],
      "metadata": {
        "id": "qrHYAhrU2Fdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test predictions\n",
        "lr_test_pred = lr_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "s5-XiNY32RFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test recall\n",
        "lr_test_recall = recall(y_test,lr_test_pred)\n",
        "lr_test_recall"
      ],
      "metadata": {
        "id": "htXhemjw2aN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test,lr_test_pred))"
      ],
      "metadata": {
        "id": "Mjl2Rp3-2eoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "lr_confusion_matrix = cm(y_test, lr_test_pred)\n",
        "cm_display = cmd(confusion_matrix = lr_confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "font = {'family' : 'DejaVu Sans',\n",
        "        'weight' : 'bold',\n",
        "        'size'   : 22}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "cm_display.plot(cmap='Blues')\n",
        "plt.title('Confusion matrix: LOGISTIC REGRESSION')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ehs0w-CJ2lVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see False Negative after applying Logistic Regression is 41."
      ],
      "metadata": {
        "id": "fklBIjxB24ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive Bayes**"
      ],
      "metadata": {
        "id": "gPYRuw5P3FOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems.\n",
        "It is mainly used in text classification that includes a high-dimensional training dataset.\n",
        "Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.\n",
        "It is a probabilistic classifier, which means it predicts on the basis of the probability of an object."
      ],
      "metadata": {
        "id": "Ff9JB7pqqE6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using stratified k fold cross validation so that each split \n",
        "cv_method = RepeatedStratifiedKFold(n_splits=4,\n",
        "                                    n_repeats=3, \n",
        "                                    random_state=0)"
      ],
      "metadata": {
        "id": "aXV6Umuf3JnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting Naive Bayes model\n",
        "nb_model = GaussianNB()"
      ],
      "metadata": {
        "id": "uEaWQt8y3Rxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Max depth of dt without hyperparameter tuning = 28 and min samples leaf = 1\n",
        "nb_model = GaussianNB()\n",
        "nb_params = {'var_smoothing': np.logspace(0,-9, num=100)\n",
        "             }"
      ],
      "metadata": {
        "id": "T1S6PjNi3aT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_gridsearch = GridSearchCV(nb_model,\n",
        "                             nb_params,\n",
        "                             cv=cv_method,\n",
        "                             scoring= 'recall')\n",
        "nb_gridsearch.fit(X_smote_scaled,y_smote)\n",
        "nb_best_params = nb_gridsearch.best_params_\n",
        "     "
      ],
      "metadata": {
        "id": "TpCMdD2gWn-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model best parameters\n",
        "nb_best_params"
      ],
      "metadata": {
        "id": "fYjx0NvCWuwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Naive Bayes model with best parameters\n",
        "nb_model = GaussianNB(var_smoothing=nb_best_params['var_smoothing'])"
      ],
      "metadata": {
        "id": "H3eeA31YW1HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "nb_model.fit(X_smote_scaled, y_smote)"
      ],
      "metadata": {
        "id": "3tZWG04iW_QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train predictions\n",
        "nb_train_pred = nb_model.predict(X_smote_scaled)"
      ],
      "metadata": {
        "id": "tgsYbBtGXJbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set recall\n",
        "nb_train_recall = recall(y_smote,nb_train_pred)\n",
        "nb_train_recall"
      ],
      "metadata": {
        "id": "ZXsNP5QoXQIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test predictions\n",
        "nb_test_pred = nb_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "DhLug5cKXXAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test recall\n",
        "nb_test_recall = recall(y_test,nb_test_pred)\n",
        "nb_test_recall"
      ],
      "metadata": {
        "id": "Xu2yRabRXcjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test,nb_test_pred))"
      ],
      "metadata": {
        "id": "U-nNI2M_XhQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "nb_confusion_matrix = cm(y_test, nb_test_pred)\n",
        "cm_display = cmd(confusion_matrix = nb_confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "font = {'family' : 'DejaVu Sans',\n",
        "        'weight' : 'bold',\n",
        "        'size'   : 22}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "cm_display.plot(cmap='Blues')\n",
        "plt.title('Confusion matrix: NAIVE BAYES')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N0IbILR4XnKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see False Negative after applying Naive Bayes is 69."
      ],
      "metadata": {
        "id": "lXR4n0auXym6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Machines**"
      ],
      "metadata": {
        "id": "qpL-z8QwYbpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane."
      ],
      "metadata": {
        "id": "QzBtFDw_py2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM model parameters\n",
        "svm_model = SVC()\n",
        "svm_params = {'C': [0.1, 1, 10],\n",
        "              'gamma': [0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf']\n",
        "             }"
      ],
      "metadata": {
        "id": "-g0RqTowXxvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using gridsearchcv to find best parameters\n",
        "svm_gridsearch = GridSearchCV(svm_model,\n",
        "                              svm_params,\n",
        "                              cv=cv_method,\n",
        "                              scoring= 'recall')\n",
        "svm_gridsearch.fit(X_smote_scaled,y_smote)\n",
        "svm_best_params = svm_gridsearch.best_params_"
      ],
      "metadata": {
        "id": "1RmYbRPOYlM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model best parameters\n",
        "svm_best_params"
      ],
      "metadata": {
        "id": "YhgkyMXhY7XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building SVM with best parameters\n",
        "svm_model = SVC(C=svm_best_params['C'],\n",
        "                gamma=svm_best_params['gamma'],\n",
        "                kernel=svm_best_params['kernel']\n",
        "                )"
      ],
      "metadata": {
        "id": "jg1bu2p9ZFAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model \n",
        "svm_model.fit(X_smote_scaled, y_smote)"
      ],
      "metadata": {
        "id": "uoTri4vnZOXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train predictions\n",
        "svm_train_pred = svm_model.predict(X_smote_scaled)"
      ],
      "metadata": {
        "id": "p5VKHy18ZUDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set recall\n",
        "svm_train_recall = recall(y_smote,svm_train_pred)\n",
        "svm_train_recall"
      ],
      "metadata": {
        "id": "7KKzv617ZaPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test predictions\n",
        "svm_test_pred = svm_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "RoeryqpRZ3hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test recall\n",
        "svm_test_recall = recall(y_test,svm_test_pred)\n",
        "svm_test_recall"
      ],
      "metadata": {
        "id": "jMw8LlD8Z-Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test,svm_test_pred))"
      ],
      "metadata": {
        "id": "Hx47NeldaDTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "svm_confusion_matrix = cm(y_test, svm_test_pred)\n",
        "cm_display = cmd(confusion_matrix = svm_confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "font = {'family' : 'DejaVu Sans',\n",
        "        'weight' : 'bold',\n",
        "        'size'   : 22}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "cm_display.plot(cmap='Blues')\n",
        "plt.title('Confusion matrix: SUPPORT VECTOR MACHINES')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fYD10CZ8aJKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see False Negative after applying Suport Vector Machine is 39. So, it is better model than above but not betetr than KNN."
      ],
      "metadata": {
        "id": "k2s4OcqbaU03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree**"
      ],
      "metadata": {
        "id": "dQPI6g7qadfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome."
      ],
      "metadata": {
        "id": "DvqTAu9andU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Max depth of Decision Tree without hyperparameter tuning \n",
        "dt_model = DecisionTreeClassifier()\n",
        "dt_params = {'max_depth':np.arange(1,10),\n",
        "             'min_samples_split':np.arange(0.1,1,0.1),\n",
        "             'min_samples_leaf':np.arange(0.1,0.6,0.1)\n",
        "             }"
      ],
      "metadata": {
        "id": "N2RppwLIauZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using gridsearchcv to find best parameters\n",
        "dt_gridsearch = GridSearchCV(dt_model,\n",
        "                             dt_params,\n",
        "                             cv=cv_method,\n",
        "                             scoring= 'recall')\n",
        "dt_gridsearch.fit(X_smote,y_smote)\n",
        "dt_best_params = dt_gridsearch.best_params_"
      ],
      "metadata": {
        "id": "vt0azxnsa96O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model best parameters\n",
        "dt_best_params"
      ],
      "metadata": {
        "id": "TPLseGRobPNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Decesion Tree with best parameters\n",
        "dt_model = DecisionTreeClassifier(max_depth=dt_best_params['max_depth'],\n",
        "                                  min_samples_split=dt_best_params['min_samples_split'],\n",
        "                                  min_samples_leaf=dt_best_params['min_samples_leaf'])"
      ],
      "metadata": {
        "id": "Nt_6QJwxbXY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model \n",
        "dt_model.fit(X_smote_scaled, y_smote)"
      ],
      "metadata": {
        "id": "4ctpW5YGbhAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train predictions\n",
        "dt_train_pred = dt_model.predict(X_smote_scaled)"
      ],
      "metadata": {
        "id": "AtvtBQzxbmJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set recall\n",
        "dt_train_recall = recall(y_smote,dt_train_pred)\n",
        "dt_train_recall"
      ],
      "metadata": {
        "id": "6acrq_svbqfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test predictions\n",
        "dt_test_pred = dt_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "4Alzf71JbwJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test recall\n",
        "dt_test_recall = recall(y_test,dt_test_pred)\n",
        "dt_test_recall"
      ],
      "metadata": {
        "id": "deCoHsFob0GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test,dt_test_pred))"
      ],
      "metadata": {
        "id": "HICw253ob4lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "dt_confusion_matrix = cm(y_test, dt_test_pred)\n",
        "cm_display = cmd(confusion_matrix = dt_confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "font = {'family' : 'DejaVu Sans',\n",
        "        'weight' : 'bold',\n",
        "        'size'   : 22}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "cm_display.plot(cmap='Blues')\n",
        "plt.title('Confusion matrix: DECISION TREE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SY-wkacpcEMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see False Negative after applying Decision Tree is 19. So, it is better model than all above models."
      ],
      "metadata": {
        "id": "WQ5HpVsdcXB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizing the best model**"
      ],
      "metadata": {
        "id": "e1NsiQQccyEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Recall scores\n",
        "\n",
        "ML_models = ['K Nearest Neighbors','Logistic Regression','Naive Bayes','Support Vector Machines','Decision Tree']\n",
        "train_recalls = [knn_train_recall,lr_train_recall,nb_train_recall,svm_train_recall,dt_train_recall]\n",
        "test_recalls = [knn_test_recall,lr_test_recall,nb_test_recall,svm_test_recall,dt_test_recall]\n",
        "  \n",
        "X_axis = np.arange(len(ML_models))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.barh(X_axis - 0.2, train_recalls, 0.4, label = 'Train Recall')\n",
        "plt.barh(X_axis + 0.2, test_recalls, 0.4, label = 'Test Recall')\n",
        "  \n",
        "plt.yticks(X_axis,ML_models)\n",
        "plt.xlabel(\"Recall score\")\n",
        "plt.title(\"Recall score for each model\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left',title='Legend')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-6_9w2YIcWxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see train recall of KNN is highest but test recall is low ,there is lot of difference between train recall and test recall. So, this model will not be the best one.\n",
        "\n",
        "From the visualization we can see that Decsion Tree has best Train and  test recall so, we can that Decision tree is the best among all these."
      ],
      "metadata": {
        "id": "LKQaACd6xBym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From our Project the conclusion are as follows:\n",
        "\n",
        "\n",
        "*   Among all the Classification model Decission Tree was found to be the best because it hs best train and test recall and False negative was 19  which was lowest among all the models and best train and test recall.\n",
        "*   From our analysis, we found that the age of a person was the most important feature in determining the risk of a patient getting with CHD, followed by pulse pressure, prevalent hypertension and total cholesterol.\n",
        "\n",
        "*   Diabetes, prevalent stroke and BP medication were the least important features in determining the risk of CHD.\n",
        "*   Recall was chosen as the model evaluation metric because it was very important that we reduce the false negatives.\n",
        "\n",
        "*   Here, to build our model we imputed missing values, we did feauture engineering and oversampled our data using SMOTE.\n",
        "*   KNN was a good model but difference between Test recall and Train recall was more so, Decision tree was found to be the best among all the models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}